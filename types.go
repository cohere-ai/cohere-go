// This file was auto-generated by Fern from our API Definition.

package api

import (
	json "encoding/json"
	fmt "fmt"
	time "time"

	core "github.com/cohere-ai/cohere-go/v2/core"
)

type ChatRequest struct {
	// Accepts a string.
	// The chat message from the user to the model.
	Message string `json:"message"`
	// Defaults to `command`.
	// The identifier of the model, which can be one of the existing Cohere models or the full ID for a [finetuned custom model](/docs/training-custom-models).
	// Compatible Cohere models are `command` and `command-light` as well as the experimental `command-nightly` and `command-light-nightly` variants. Read more about [Cohere models](https://docs.cohere.com/docs/models).
	Model *string `json:"model,omitempty"`
	// When specified, the default Cohere preamble will be replaced with the provided one.
	PreambleOverride *string `json:"preamble_override,omitempty"`
	// A list of previous messages between the user and the model, meant to give the model conversational context for responding to the user's `message`.
	ChatHistory []*ChatMessage `json:"chat_history,omitempty"`
	// An alternative to `chat_history`. Previous conversations can be resumed by providing the conversation's identifier. The contents of `message` and the model's response will be stored as part of this conversation.
	// If a conversation with this id does not already exist, a new conversation will be created.
	ConversationId *string `json:"conversation_id,omitempty"`
	// Defaults to `AUTO` when `connectors` are specified and `OFF` in all other cases.
	// Dictates how the prompt will be constructed.
	// With `prompt_truncation` set to "AUTO", some elements from `chat_history` and `documents` will be dropped in an attempt to construct a prompt that fits within the model's context length limit.
	// With `prompt_truncation` set to "OFF", no elements will be dropped. If the sum of the inputs exceeds the model's context length limit, a `TooManyTokens` error will be returned.
	PromptTruncation *ChatRequestPromptTruncation `json:"prompt_truncation,omitempty"`
	// Accepts `{"id": "web-search"}`, and/or the `"id"` for a custom connector, if you've made one.
	// When specified, the model's reply will be enriched with information found by quering each of the connectors (RAG).
	Connectors []*ChatConnector `json:"connectors,omitempty"`
	// Defaults to `false`.
	// When `true`, the response will only contain a list of generated search queries, but no search will take place, and no reply from the model to the user's `message` will be generated.
	SearchQueriesOnly *bool `json:"search_queries_only,omitempty"`
	// A list of relevant documents that the model can use to enrich its reply. See ['Document Mode'](https://docs.cohere.com/docs/retrieval-augmented-generation-rag#document-mode) in the guide for more information.
	Documents []ChatDocument `json:"documents,omitempty"`
	// Defaults to `"accurate"`.
	// Dictates the approach taken to generating citations as part of the RAG flow by allowing the user to specify whether they want `"accurate"` results or `"fast"` results.
	CitationQuality *ChatRequestCitationQuality `json:"citation_quality,omitempty"`
	// Defaults to `0.3`
	// A non-negative float that tunes the degree of randomness in generation. Lower temperatures mean less random generations, and higher temperatures mean more random generations.
	Temperature *float64 `json:"temperature,omitempty"`
	stream      bool
}

func (c *ChatRequest) Stream() bool {
	return c.stream
}

func (c *ChatRequest) UnmarshalJSON(data []byte) error {
	type unmarshaler ChatRequest
	var body unmarshaler
	if err := json.Unmarshal(data, &body); err != nil {
		return err
	}
	*c = ChatRequest(body)
	c.stream = false
	return nil
}

func (c *ChatRequest) MarshalJSON() ([]byte, error) {
	type embed ChatRequest
	var marshaler = struct {
		embed
		Stream bool `json:"stream"`
	}{
		embed:  embed(*c),
		Stream: false,
	}
	return json.Marshal(marshaler)
}

type ChatStreamRequest struct {
	// Accepts a string.
	// The chat message from the user to the model.
	Message string `json:"message"`
	// Defaults to `command`.
	// The identifier of the model, which can be one of the existing Cohere models or the full ID for a [finetuned custom model](/docs/training-custom-models).
	// Compatible Cohere models are `command` and `command-light` as well as the experimental `command-nightly` and `command-light-nightly` variants. Read more about [Cohere models](https://docs.cohere.com/docs/models).
	Model *string `json:"model,omitempty"`
	// When specified, the default Cohere preamble will be replaced with the provided one.
	PreambleOverride *string `json:"preamble_override,omitempty"`
	// A list of previous messages between the user and the model, meant to give the model conversational context for responding to the user's `message`.
	ChatHistory []*ChatMessage `json:"chat_history,omitempty"`
	// An alternative to `chat_history`. Previous conversations can be resumed by providing the conversation's identifier. The contents of `message` and the model's response will be stored as part of this conversation.
	// If a conversation with this id does not already exist, a new conversation will be created.
	ConversationId *string `json:"conversation_id,omitempty"`
	// Defaults to `AUTO` when `connectors` are specified and `OFF` in all other cases.
	// Dictates how the prompt will be constructed.
	// With `prompt_truncation` set to "AUTO", some elements from `chat_history` and `documents` will be dropped in an attempt to construct a prompt that fits within the model's context length limit.
	// With `prompt_truncation` set to "OFF", no elements will be dropped. If the sum of the inputs exceeds the model's context length limit, a `TooManyTokens` error will be returned.
	PromptTruncation *ChatStreamRequestPromptTruncation `json:"prompt_truncation,omitempty"`
	// Accepts `{"id": "web-search"}`, and/or the `"id"` for a custom connector, if you've made one.
	// When specified, the model's reply will be enriched with information found by quering each of the connectors (RAG).
	Connectors []*ChatConnector `json:"connectors,omitempty"`
	// Defaults to `false`.
	// When `true`, the response will only contain a list of generated search queries, but no search will take place, and no reply from the model to the user's `message` will be generated.
	SearchQueriesOnly *bool `json:"search_queries_only,omitempty"`
	// A list of relevant documents that the model can use to enrich its reply. See ['Document Mode'](https://docs.cohere.com/docs/retrieval-augmented-generation-rag#document-mode) in the guide for more information.
	Documents []ChatDocument `json:"documents,omitempty"`
	// Defaults to `"accurate"`.
	// Dictates the approach taken to generating citations as part of the RAG flow by allowing the user to specify whether they want `"accurate"` results or `"fast"` results.
	CitationQuality *ChatStreamRequestCitationQuality `json:"citation_quality,omitempty"`
	// Defaults to `0.3`
	// A non-negative float that tunes the degree of randomness in generation. Lower temperatures mean less random generations, and higher temperatures mean more random generations.
	Temperature *float64 `json:"temperature,omitempty"`
	stream      bool
}

func (c *ChatStreamRequest) Stream() bool {
	return c.stream
}

func (c *ChatStreamRequest) UnmarshalJSON(data []byte) error {
	type unmarshaler ChatStreamRequest
	var body unmarshaler
	if err := json.Unmarshal(data, &body); err != nil {
		return err
	}
	*c = ChatStreamRequest(body)
	c.stream = true
	return nil
}

func (c *ChatStreamRequest) MarshalJSON() ([]byte, error) {
	type embed ChatStreamRequest
	var marshaler = struct {
		embed
		Stream bool `json:"stream"`
	}{
		embed:  embed(*c),
		Stream: true,
	}
	return json.Marshal(marshaler)
}

type ClassifyRequest struct {
	// Represents a list of queries to be classified, each entry must not be empty. The maximum is 96 inputs.
	Inputs []string `json:"inputs,omitempty"`
	// An array of examples to provide context to the model. Each example is a text string and its associated label/class. Each unique label requires at least 2 examples associated with it; the maximum number of examples is 2500, and each example has a maximum length of 512 tokens. The values should be structured as `{text: "...",label: "..."}`.
	// Note: [Custom Models](/training-representation-models) trained on classification examples don't require the `examples` parameter to be passed in explicitly.
	Examples []*ClassifyRequestExamplesItem `json:"examples,omitempty"`
	// The identifier of the model. Currently available models are `embed-multilingual-v2.0`, `embed-english-light-v2.0`, and `embed-english-v2.0` (default). Smaller "light" models are faster, while larger models will perform better. [Custom models](/docs/training-custom-models) can also be supplied with their full ID.
	Model *string `json:"model,omitempty"`
	// The ID of a custom playground preset. You can create presets in the [playground](https://dashboard.cohere.ai/playground/classify?model=large). If you use a preset, all other parameters become optional, and any included parameters will override the preset's parameters.
	Preset *string `json:"preset,omitempty"`
	// One of `NONE|START|END` to specify how the API will handle inputs longer than the maximum token length.
	// Passing `START` will discard the start of the input. `END` will discard the end of the input. In both cases, input is discarded until the remaining input is exactly the maximum input token length for the model.
	// If `NONE` is selected, when the input exceeds the maximum input token length an error will be returned.
	Truncate *ClassifyRequestTruncate `json:"truncate,omitempty"`
}

type CreateClusterJobRequest struct {
	Required       interface{} `json:"required,omitempty"`
	EmbeddingsUrl  *string     `json:"embeddings_url,omitempty"`
	InputDatasetId *string     `json:"input_dataset_id,omitempty"`
	// Defaults to `10`. Parameter for HDBSCAN. Only clusters with this number of elements will be returned with a positive cluster number.
	MinClusterSize *int `json:"min_cluster_size,omitempty"`
	// Parameter for UMAP. A scalar governing how to balance global vs local structure in the data.
	NNeighbors *int `json:"n_neighbors,omitempty"`
	// Parameter for UMAP. A boolean governing whether the embeddings from UMAP (that will be clustered with HDBSCAN) are deterministic.
	IsDeterministic      *bool `json:"is_deterministic,omitempty"`
	GenerateDescriptions *bool `json:"generate_descriptions,omitempty"`
}

type DetectLanguageRequest struct {
	// List of strings to run the detection on.
	Texts []string `json:"texts,omitempty"`
	// The identifier of the model to generate with.
	Model *string `json:"model,omitempty"`
}

type DetokenizeRequest struct {
	// The list of tokens to be detokenized.
	Tokens []int `json:"tokens,omitempty"`
	// An optional parameter to provide the model name. This will ensure that the detokenization is done by the tokenizer used by that model.
	Model *string `json:"model,omitempty"`
}

type EmbedRequest struct {
	// An array of strings for the model to embed. Maximum number of texts per call is `96`. We recommend reducing the length of each text to be under `512` tokens for optimal quality.
	Texts []string `json:"texts,omitempty"`
	// Defaults to embed-english-v2.0
	//
	// The identifier of the model. Smaller "light" models are faster, while larger models will perform better. [Custom models](/docs/training-custom-models) can also be supplied with their full ID.
	//
	// Available models and corresponding embedding dimensions:
	//
	// * `embed-english-v3.0`  1024
	// * `embed-multilingual-v3.0`  1024
	// * `embed-english-light-v3.0`  384
	// * `embed-multilingual-light-v3.0`  384
	//
	// * `embed-english-v2.0`  4096
	// * `embed-english-light-v2.0`  1024
	// * `embed-multilingual-v2.0`  768
	Model *string `json:"model,omitempty"`
	// Specifies the type of input you're giving to the model. Not required for older versions of the embedding models (i.e. anything lower than v3), but is required for more recent versions (i.e. anything bigger than v2).
	//
	// * `"search_document"`: Use this when you encode documents for embeddings that you store in a vector database for search use-cases.
	// * `"search_query"`: Use this when you query your vector DB to find relevant documents.
	// * `"classification"`: Use this when you use the embeddings as an input to a text classifier.
	// * `"clustering"`: Use this when you want to cluster the embeddings.
	InputType *string `json:"input_type,omitempty"`
	// One of `NONE|START|END` to specify how the API will handle inputs longer than the maximum token length.
	//
	// Passing `START` will discard the start of the input. `END` will discard the end of the input. In both cases, input is discarded until the remaining input is exactly the maximum input token length for the model.
	//
	// If `NONE` is selected, when the input exceeds the maximum input token length an error will be returned.
	Truncate *EmbedRequestTruncate `json:"truncate,omitempty"`
}

type GenerateRequest struct {
	// The input text that serves as the starting point for generating the response.
	// Note: The prompt will be pre-processed and modified before reaching the model.
	Prompt string `json:"prompt"`
	// The identifier of the model to generate with. Currently available models are `command` (default), `command-nightly` (experimental), `command-light`, and `command-light-nightly` (experimental).
	// Smaller, "light" models are faster, while larger models will perform better. [Custom models](/docs/training-custom-models) can also be supplied with their full ID.
	Model *string `json:"model,omitempty"`
	// The maximum number of generations that will be returned. Defaults to `1`, min value of `1`, max value of `5`.
	NumGenerations *int `json:"num_generations,omitempty"`
	// When `true`, the response will be a JSON stream of events. Streaming is beneficial for user interfaces that render the contents of the response piece by piece, as it gets generated.
	//
	// The final event will contain the complete response, and will contain an `is_finished` field set to `true`. The event will also contain a `finish_reason`, which can be one of the following:
	// - `COMPLETE` - the model sent back a finished reply
	// - `MAX_TOKENS` - the reply was cut off because the model reached the maximum number of tokens for its context length
	// - `ERROR` - something went wrong when generating the reply
	// - `ERROR_TOXIC` - the model generated a reply that was deemed toxic
	Stream *bool `json:"stream,omitempty"`
	// The maximum number of tokens the model will generate as part of the response. Note: Setting a low value may result in incomplete generations.
	//
	// This parameter is off by default, and if it's not specified, the model will continue generating until it emits an EOS completion token. See [BPE Tokens](/bpe-tokens-wiki) for more details.
	//
	// Can only be set to `0` if `return_likelihoods` is set to `ALL` to get the likelihood of the prompt.
	MaxTokens *int `json:"max_tokens,omitempty"`
	// One of `NONE|START|END` to specify how the API will handle inputs longer than the maximum token length.
	//
	// Passing `START` will discard the start of the input. `END` will discard the end of the input. In both cases, input is discarded until the remaining input is exactly the maximum input token length for the model.
	//
	// If `NONE` is selected, when the input exceeds the maximum input token length an error will be returned.
	Truncate *GenerateRequestTruncate `json:"truncate,omitempty"`
	// A non-negative float that tunes the degree of randomness in generation. Lower temperatures mean less random generations. See [Temperature](/temperature-wiki) for more details.
	// Defaults to `0.75`, min value of `0.0`, max value of `5.0`.
	Temperature *float64 `json:"temperature,omitempty"`
	// Identifier of a custom preset. A preset is a combination of parameters, such as prompt, temperature etc. You can create presets in the [playground](https://dashboard.cohere.ai/playground/generate).
	// When a preset is specified, the `prompt` parameter becomes optional, and any included parameters will override the preset's parameters.
	Preset *string `json:"preset,omitempty"`
	// The generated text will be cut at the beginning of the earliest occurrence of an end sequence. The sequence will be excluded from the text.
	EndSequences []string `json:"end_sequences,omitempty"`
	// The generated text will be cut at the end of the earliest occurrence of a stop sequence. The sequence will be included the text.
	StopSequences []string `json:"stop_sequences,omitempty"`
	// Ensures only the top `k` most likely tokens are considered for generation at each step.
	// Defaults to `0`, min value of `0`, max value of `500`.
	K *int `json:"k,omitempty"`
	// Ensures that only the most likely tokens, with total probability mass of `p`, are considered for generation at each step. If both `k` and `p` are enabled, `p` acts after `k`.
	// Defaults to `0`. min value of `0.01`, max value of `0.99`.
	P *float64 `json:"p,omitempty"`
	// Used to reduce repetitiveness of generated tokens. The higher the value, the stronger a penalty is applied to previously present tokens, proportional to how many times they have already appeared in the prompt or prior generation.'
	FrequencyPenalty *float64 `json:"frequency_penalty,omitempty"`
	// Defaults to `0.0`, min value of `0.0`, max value of `1.0`. Can be used to reduce repetitiveness of generated tokens. Similar to `frequency_penalty`, except that this penalty is applied equally to all tokens that have already appeared, regardless of their exact frequencies.
	PresencePenalty *float64 `json:"presence_penalty,omitempty"`
	// One of `GENERATION|ALL|NONE` to specify how and if the token likelihoods are returned with the response. Defaults to `NONE`.
	//
	// If `GENERATION` is selected, the token likelihoods will only be provided for generated text.
	//
	// If `ALL` is selected, the token likelihoods will be provided both for the prompt and the generated text.
	ReturnLikelihoods *GenerateRequestReturnLikelihoods `json:"return_likelihoods,omitempty"`
	// Used to prevent the model from generating unwanted tokens or to incentivize it to include desired tokens. The format is `{token_id: bias}` where bias is a float between -10 and 10. Tokens can be obtained from text using [Tokenize](/reference/tokenize).
	//
	// For example, if the value `{'11': -10}` is provided, the model will be very unlikely to include the token 11 (`"\n"`, the newline character) anywhere in the generated text. In contrast `{'11': 10}` will result in generations that nearly only contain that token. Values between -10 and 10 will proportionally affect the likelihood of the token appearing in the generated text.
	//
	// Note: logit bias may not be supported for all custom models.
	LogitBias map[string]float64 `json:"logit_bias,omitempty"`
}

type LoglikelihoodRequest struct {
	// The identifier of the model to generate with. Currently available models are `command` (default), `command-nightly` (experimental), `command-light`, and `command-light-nightly` (experimental). Smaller, "light" models are faster, while larger models will perform better. [Custom models](/docs/training-custom-models) can also be supplied with their full ID.
	Model *string `json:"model,omitempty"`
	// To be used in conjunction with `completion`. This will be interpolated into the user message in the prompt template.
	Prompt *string `json:"prompt,omitempty"`
	// To be used in conjunction with `prompt`. This will be interpolated into the chatbot reponse in the prompt template.
	Completion *string `json:"completion,omitempty"`
	// To be used on its own, this allows you to pass a custom prompt to the model that will not be interpolated into a prompt template.
	RawPrompt *string `json:"raw_prompt,omitempty"`
}

type RerankRequest struct {
	// The identifier of the model to use, one of : `rerank-english-v2.0`, `rerank-multilingual-v2.0`
	Model *string `json:"model,omitempty"`
	// The search query
	Query string `json:"query"`
	// A list of document objects or strings to rerank.
	// If a document is provided the text fields is required and all other fields will be preserved in the response.
	//
	// The total max chunks (length of documents * max_chunks_per_doc) must be less than 10000.
	//
	// We recommend a maximum of 1,000 documents for optimal endpoint performance.
	Documents []*RerankRequestDocumentsItem `json:"documents,omitempty"`
	// The number of most relevant documents or indices to return, defaults to the length of the documents
	TopN *int `json:"top_n,omitempty"`
	// - If false, returns results without the doc text - the api will return a list of {index, relevance score} where index is inferred from the list passed into the request.
	// - If true, returns results with the doc text passed in - the api will return an ordered list of {index, text, relevance score} where index + text refers to the list passed into the request.
	ReturnDocuments *bool `json:"return_documents,omitempty"`
	// The maximum number of chunks to produce internally from a document
	MaxChunksPerDoc *int `json:"max_chunks_per_doc,omitempty"`
}

type SummarizeRequest struct {
	// The text to generate a summary for. Can be up to 100,000 characters long. Currently the only supported language is English.
	Text string `json:"text"`
	// One of `short`, `medium`, `long`, or `auto` defaults to `auto`. Indicates the approximate length of the summary. If `auto` is selected, the best option will be picked based on the input text.
	Length *SummarizeRequestLength `json:"length,omitempty"`
	// One of `paragraph`, `bullets`, or `auto`, defaults to `auto`. Indicates the style in which the summary will be delivered - in a free form paragraph or in bullet points. If `auto` is selected, the best option will be picked based on the input text.
	Format *SummarizeRequestFormat `json:"format,omitempty"`
	// The identifier of the model to generate the summary with. Currently available models are `command` (default), `command-nightly` (experimental), `command-light`, and `command-light-nightly` (experimental). Smaller, "light" models are faster, while larger models will perform better.
	Model *string `json:"model,omitempty"`
	// One of `low`, `medium`, `high`, or `auto`, defaults to `auto`. Controls how close to the original text the summary is. `high` extractiveness summaries will lean towards reusing sentences verbatim, while `low` extractiveness summaries will tend to paraphrase more. If `auto` is selected, the best option will be picked based on the input text.
	Extractiveness *SummarizeRequestExtractiveness `json:"extractiveness,omitempty"`
	// Ranges from 0 to 5. Controls the randomness of the output. Lower values tend to generate more “predictable” output, while higher values tend to generate more “creative” output. The sweet spot is typically between 0 and 1.
	Temperature *float64 `json:"temperature,omitempty"`
	// A free-form instruction for modifying how the summaries get generated. Should complete the sentence "Generate a summary _". Eg. "focusing on the next steps" or "written by Yoda"
	AdditionalCommand *string `json:"additional_command,omitempty"`
}

type TokenizeRequest struct {
	// The string to be tokenized, the minimum text length is 1 character, and the maximum text length is 65536 characters.
	Text string `json:"text"`
	// An optional parameter to provide the model name. This will ensure that the tokenization uses the tokenizer used by that model.
	Model *string `json:"model,omitempty"`
}

type ApiMeta struct {
	ApiVersion *ApiMetaApiVersion `json:"api_version,omitempty"`
	Warnings   []string           `json:"warnings,omitempty"`

	_rawJSON json.RawMessage
}

func (a *ApiMeta) UnmarshalJSON(data []byte) error {
	type unmarshaler ApiMeta
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*a = ApiMeta(value)
	a._rawJSON = json.RawMessage(data)
	return nil
}

func (a *ApiMeta) String() string {
	if len(a._rawJSON) > 0 {
		if value, err := core.StringifyJSON(a._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(a); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", a)
}

type ApiMetaApiVersion struct {
	Version        string                        `json:"version"`
	IsDeprecated   *bool                         `json:"is_deprecated,omitempty"`
	IsExperimental *bool                         `json:"is_experimental,omitempty"`
	BilledUnits    *ApiMetaApiVersionBilledUnits `json:"billed_units,omitempty"`

	_rawJSON json.RawMessage
}

func (a *ApiMetaApiVersion) UnmarshalJSON(data []byte) error {
	type unmarshaler ApiMetaApiVersion
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*a = ApiMetaApiVersion(value)
	a._rawJSON = json.RawMessage(data)
	return nil
}

func (a *ApiMetaApiVersion) String() string {
	if len(a._rawJSON) > 0 {
		if value, err := core.StringifyJSON(a._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(a); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", a)
}

type ApiMetaApiVersionBilledUnits struct {
	// The number of billed input tokens.
	InputTokens *float64 `json:"input_tokens,omitempty"`
	// The number of billed output tokens.
	OutputTokens *float64 `json:"output_tokens,omitempty"`
	// The number of billed search units.
	SearchUnits *float64 `json:"search_units,omitempty"`
	// The number of billed classifications units.
	Classifications *float64 `json:"classifications,omitempty"`

	_rawJSON json.RawMessage
}

func (a *ApiMetaApiVersionBilledUnits) UnmarshalJSON(data []byte) error {
	type unmarshaler ApiMetaApiVersionBilledUnits
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*a = ApiMetaApiVersionBilledUnits(value)
	a._rawJSON = json.RawMessage(data)
	return nil
}

func (a *ApiMetaApiVersionBilledUnits) String() string {
	if len(a._rawJSON) > 0 {
		if value, err := core.StringifyJSON(a._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(a); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", a)
}

// A section of the generated reply which cites external knowledge.
type ChatCitation struct {
	// The index of text that the citation starts at, counting from zero. For example, a generation of `Hello, world!` with a citation on `world` would have a start value of `7`. This is because the citation starts at `w`, which is the seventh character.
	Start int `json:"start"`
	// The index of text that the citation ends after, counting from zero. For example, a generation of `Hello, world!` with a citation on `world` would have an end value of `11`. This is because the citation ends after `d`, which is the eleventh character.
	End int `json:"end"`
	// The text of the citation. For example, a generation of `Hello, world!` with a citation of `world` would have a text value of `world`.
	Text string `json:"text"`
	// Identifiers of documents cited by this section of the generated reply.
	DocumentIds []string `json:"document_ids,omitempty"`

	_rawJSON json.RawMessage
}

func (c *ChatCitation) UnmarshalJSON(data []byte) error {
	type unmarshaler ChatCitation
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*c = ChatCitation(value)
	c._rawJSON = json.RawMessage(data)
	return nil
}

func (c *ChatCitation) String() string {
	if len(c._rawJSON) > 0 {
		if value, err := core.StringifyJSON(c._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(c); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", c)
}

type ChatCitationGenerationEvent struct {
	// Citations for the generated reply.
	Citations []*ChatCitation `json:"citations,omitempty"`

	_rawJSON json.RawMessage
}

func (c *ChatCitationGenerationEvent) UnmarshalJSON(data []byte) error {
	type unmarshaler ChatCitationGenerationEvent
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*c = ChatCitationGenerationEvent(value)
	c._rawJSON = json.RawMessage(data)
	return nil
}

func (c *ChatCitationGenerationEvent) String() string {
	if len(c._rawJSON) > 0 {
		if value, err := core.StringifyJSON(c._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(c); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", c)
}

// The connector used for fetching documents.
type ChatConnector struct {
	// The identifier of the connector. Currently only 'web-search' is supported.
	Id string `json:"id"`
	// Provides the connector with different settings at request time. The key/value pairs of this object are specific to each connector.
	//
	// The supported options are:
	//
	// **web-search**
	//
	// **site** - The web search results will be restricted to this domain (and TLD) when specified. Only a single domain is specified, and subdomains are also accepted.
	// Examples:
	// * `{"options": {"site": "cohere.com"}}` would restrict the results to all subdomains at cohere.com
	// * `{"options": {"site": "txt.cohere.com"}}` would restrict the results to `txt.cohere.com`
	Options map[string]interface{} `json:"options,omitempty"`

	_rawJSON json.RawMessage
}

func (c *ChatConnector) UnmarshalJSON(data []byte) error {
	type unmarshaler ChatConnector
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*c = ChatConnector(value)
	c._rawJSON = json.RawMessage(data)
	return nil
}

func (c *ChatConnector) String() string {
	if len(c._rawJSON) > 0 {
		if value, err := core.StringifyJSON(c._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(c); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", c)
}

// Relevant information that could be used by the model to generate a more accurate reply.
// The contents of each document are generally short (under 300 words), and are passed in the form of a
// dictionary of strings. Some suggested keys are "text", "author", "date". Both the key name and the value will be
// passed to the model.
type ChatDocument = map[string]string

// A single message in a chat history. Contains the role of the sender, the text contents of the message, and optionally a username.
type ChatMessage struct {
	Role     ChatMessageRole `json:"role,omitempty"`
	Message  string          `json:"message"`
	UserName *string         `json:"user_name,omitempty"`

	_rawJSON json.RawMessage
}

func (c *ChatMessage) UnmarshalJSON(data []byte) error {
	type unmarshaler ChatMessage
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*c = ChatMessage(value)
	c._rawJSON = json.RawMessage(data)
	return nil
}

func (c *ChatMessage) String() string {
	if len(c._rawJSON) > 0 {
		if value, err := core.StringifyJSON(c._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(c); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", c)
}

type ChatMessageRole string

const (
	ChatMessageRoleChatbot ChatMessageRole = "CHATBOT"
	ChatMessageRoleUser    ChatMessageRole = "USER"
)

func NewChatMessageRoleFromString(s string) (ChatMessageRole, error) {
	switch s {
	case "CHATBOT":
		return ChatMessageRoleChatbot, nil
	case "USER":
		return ChatMessageRoleUser, nil
	}
	var t ChatMessageRole
	return "", fmt.Errorf("%s is not a valid %T", s, t)
}

func (c ChatMessageRole) Ptr() *ChatMessageRole {
	return &c
}

// Defaults to `"accurate"`.
// Dictates the approach taken to generating citations as part of the RAG flow by allowing the user to specify whether they want `"accurate"` results or `"fast"` results.
type ChatRequestCitationQuality string

const (
	ChatRequestCitationQualityFast     ChatRequestCitationQuality = "fast"
	ChatRequestCitationQualityAccurate ChatRequestCitationQuality = "accurate"
)

func NewChatRequestCitationQualityFromString(s string) (ChatRequestCitationQuality, error) {
	switch s {
	case "fast":
		return ChatRequestCitationQualityFast, nil
	case "accurate":
		return ChatRequestCitationQualityAccurate, nil
	}
	var t ChatRequestCitationQuality
	return "", fmt.Errorf("%s is not a valid %T", s, t)
}

func (c ChatRequestCitationQuality) Ptr() *ChatRequestCitationQuality {
	return &c
}

// Defaults to `AUTO` when `connectors` are specified and `OFF` in all other cases.
// Dictates how the prompt will be constructed.
// With `prompt_truncation` set to "AUTO", some elements from `chat_history` and `documents` will be dropped in an attempt to construct a prompt that fits within the model's context length limit.
// With `prompt_truncation` set to "OFF", no elements will be dropped. If the sum of the inputs exceeds the model's context length limit, a `TooManyTokens` error will be returned.
type ChatRequestPromptTruncation string

const (
	ChatRequestPromptTruncationOff  ChatRequestPromptTruncation = "OFF"
	ChatRequestPromptTruncationAuto ChatRequestPromptTruncation = "AUTO"
)

func NewChatRequestPromptTruncationFromString(s string) (ChatRequestPromptTruncation, error) {
	switch s {
	case "OFF":
		return ChatRequestPromptTruncationOff, nil
	case "AUTO":
		return ChatRequestPromptTruncationAuto, nil
	}
	var t ChatRequestPromptTruncation
	return "", fmt.Errorf("%s is not a valid %T", s, t)
}

func (c ChatRequestPromptTruncation) Ptr() *ChatRequestPromptTruncation {
	return &c
}

type ChatSearchQueriesGenerationEvent struct {
	// Generated search queries, meant to be used as part of the RAG flow.
	SearchQueries []*ChatSearchQuery `json:"search_queries,omitempty"`

	_rawJSON json.RawMessage
}

func (c *ChatSearchQueriesGenerationEvent) UnmarshalJSON(data []byte) error {
	type unmarshaler ChatSearchQueriesGenerationEvent
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*c = ChatSearchQueriesGenerationEvent(value)
	c._rawJSON = json.RawMessage(data)
	return nil
}

func (c *ChatSearchQueriesGenerationEvent) String() string {
	if len(c._rawJSON) > 0 {
		if value, err := core.StringifyJSON(c._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(c); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", c)
}

// The generated search query. Contains the text of the query and a unique identifier for the query.
type ChatSearchQuery struct {
	// The text of the search query.
	Text string `json:"text"`
	// Unique identifier for the generated search query. Useful for submitting feedback.
	GenerationId string `json:"generation_id"`

	_rawJSON json.RawMessage
}

func (c *ChatSearchQuery) UnmarshalJSON(data []byte) error {
	type unmarshaler ChatSearchQuery
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*c = ChatSearchQuery(value)
	c._rawJSON = json.RawMessage(data)
	return nil
}

func (c *ChatSearchQuery) String() string {
	if len(c._rawJSON) > 0 {
		if value, err := core.StringifyJSON(c._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(c); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", c)
}

type ChatSearchResult struct {
	SearchQuery *ChatSearchQuery `json:"search_query,omitempty"`
	// The connector from which this result comes from.
	Connector *ChatConnector `json:"connector,omitempty"`
	// Identifiers of documents found by this search query.
	DocumentIds []string `json:"document_ids,omitempty"`

	_rawJSON json.RawMessage
}

func (c *ChatSearchResult) UnmarshalJSON(data []byte) error {
	type unmarshaler ChatSearchResult
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*c = ChatSearchResult(value)
	c._rawJSON = json.RawMessage(data)
	return nil
}

func (c *ChatSearchResult) String() string {
	if len(c._rawJSON) > 0 {
		if value, err := core.StringifyJSON(c._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(c); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", c)
}

type ChatSearchResultsEvent struct {
	// Conducted searches and the ids of documents retrieved from each of them.
	SearchResults []*ChatSearchResult `json:"search_results,omitempty"`
	// Documents fetched from searches or provided by the user.
	Documents []ChatDocument `json:"documents,omitempty"`

	_rawJSON json.RawMessage
}

func (c *ChatSearchResultsEvent) UnmarshalJSON(data []byte) error {
	type unmarshaler ChatSearchResultsEvent
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*c = ChatSearchResultsEvent(value)
	c._rawJSON = json.RawMessage(data)
	return nil
}

func (c *ChatSearchResultsEvent) String() string {
	if len(c._rawJSON) > 0 {
		if value, err := core.StringifyJSON(c._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(c); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", c)
}

type ChatStreamEndEvent struct {
	// - `COMPLETE` - the model sent back a finished reply
	// - `ERROR_LIMIT` - the reply was cut off because the model reached the maximum number of tokens for its context length
	// - `MAX_TOKENS` - the reply was cut off because the model reached the maximum number of tokens specified by the max_tokens parameter
	// - `ERROR` - something went wrong when generating the reply
	// - `ERROR_TOXIC` - the model generated a reply that was deemed toxic
	FinishReason ChatStreamEndEventFinishReason `json:"finish_reason,omitempty"`
	// The consolidated response from the model. Contains the generated reply and all the other information streamed back in the previous events.
	Response *ChatStreamEndEventResponse `json:"response,omitempty"`

	_rawJSON json.RawMessage
}

func (c *ChatStreamEndEvent) UnmarshalJSON(data []byte) error {
	type unmarshaler ChatStreamEndEvent
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*c = ChatStreamEndEvent(value)
	c._rawJSON = json.RawMessage(data)
	return nil
}

func (c *ChatStreamEndEvent) String() string {
	if len(c._rawJSON) > 0 {
		if value, err := core.StringifyJSON(c._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(c); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", c)
}

// - `COMPLETE` - the model sent back a finished reply
// - `ERROR_LIMIT` - the reply was cut off because the model reached the maximum number of tokens for its context length
// - `MAX_TOKENS` - the reply was cut off because the model reached the maximum number of tokens specified by the max_tokens parameter
// - `ERROR` - something went wrong when generating the reply
// - `ERROR_TOXIC` - the model generated a reply that was deemed toxic
type ChatStreamEndEventFinishReason string

const (
	ChatStreamEndEventFinishReasonComplete   ChatStreamEndEventFinishReason = "COMPLETE"
	ChatStreamEndEventFinishReasonErrorLimit ChatStreamEndEventFinishReason = "ERROR_LIMIT"
	ChatStreamEndEventFinishReasonMaxTokens  ChatStreamEndEventFinishReason = "MAX_TOKENS"
	ChatStreamEndEventFinishReasonError      ChatStreamEndEventFinishReason = "ERROR"
	ChatStreamEndEventFinishReasonErrorToxic ChatStreamEndEventFinishReason = "ERROR_TOXIC"
)

func NewChatStreamEndEventFinishReasonFromString(s string) (ChatStreamEndEventFinishReason, error) {
	switch s {
	case "COMPLETE":
		return ChatStreamEndEventFinishReasonComplete, nil
	case "ERROR_LIMIT":
		return ChatStreamEndEventFinishReasonErrorLimit, nil
	case "MAX_TOKENS":
		return ChatStreamEndEventFinishReasonMaxTokens, nil
	case "ERROR":
		return ChatStreamEndEventFinishReasonError, nil
	case "ERROR_TOXIC":
		return ChatStreamEndEventFinishReasonErrorToxic, nil
	}
	var t ChatStreamEndEventFinishReason
	return "", fmt.Errorf("%s is not a valid %T", s, t)
}

func (c ChatStreamEndEventFinishReason) Ptr() *ChatStreamEndEventFinishReason {
	return &c
}

// The consolidated response from the model. Contains the generated reply and all the other information streamed back in the previous events.
type ChatStreamEndEventResponse struct {
	typeName                  string
	NonStreamedChatResponse   *NonStreamedChatResponse
	SearchQueriesOnlyResponse *SearchQueriesOnlyResponse
}

func NewChatStreamEndEventResponseFromNonStreamedChatResponse(value *NonStreamedChatResponse) *ChatStreamEndEventResponse {
	return &ChatStreamEndEventResponse{typeName: "nonStreamedChatResponse", NonStreamedChatResponse: value}
}

func NewChatStreamEndEventResponseFromSearchQueriesOnlyResponse(value *SearchQueriesOnlyResponse) *ChatStreamEndEventResponse {
	return &ChatStreamEndEventResponse{typeName: "searchQueriesOnlyResponse", SearchQueriesOnlyResponse: value}
}

func (c *ChatStreamEndEventResponse) UnmarshalJSON(data []byte) error {
	valueNonStreamedChatResponse := new(NonStreamedChatResponse)
	if err := json.Unmarshal(data, &valueNonStreamedChatResponse); err == nil {
		c.typeName = "nonStreamedChatResponse"
		c.NonStreamedChatResponse = valueNonStreamedChatResponse
		return nil
	}
	valueSearchQueriesOnlyResponse := new(SearchQueriesOnlyResponse)
	if err := json.Unmarshal(data, &valueSearchQueriesOnlyResponse); err == nil {
		c.typeName = "searchQueriesOnlyResponse"
		c.SearchQueriesOnlyResponse = valueSearchQueriesOnlyResponse
		return nil
	}
	return fmt.Errorf("%s cannot be deserialized as a %T", data, c)
}

func (c ChatStreamEndEventResponse) MarshalJSON() ([]byte, error) {
	switch c.typeName {
	default:
		return nil, fmt.Errorf("invalid type %s in %T", c.typeName, c)
	case "nonStreamedChatResponse":
		return json.Marshal(c.NonStreamedChatResponse)
	case "searchQueriesOnlyResponse":
		return json.Marshal(c.SearchQueriesOnlyResponse)
	}
}

type ChatStreamEndEventResponseVisitor interface {
	VisitNonStreamedChatResponse(*NonStreamedChatResponse) error
	VisitSearchQueriesOnlyResponse(*SearchQueriesOnlyResponse) error
}

func (c *ChatStreamEndEventResponse) Accept(visitor ChatStreamEndEventResponseVisitor) error {
	switch c.typeName {
	default:
		return fmt.Errorf("invalid type %s in %T", c.typeName, c)
	case "nonStreamedChatResponse":
		return visitor.VisitNonStreamedChatResponse(c.NonStreamedChatResponse)
	case "searchQueriesOnlyResponse":
		return visitor.VisitSearchQueriesOnlyResponse(c.SearchQueriesOnlyResponse)
	}
}

type ChatStreamEvent struct {
	_rawJSON json.RawMessage
}

func (c *ChatStreamEvent) UnmarshalJSON(data []byte) error {
	type unmarshaler ChatStreamEvent
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*c = ChatStreamEvent(value)
	c._rawJSON = json.RawMessage(data)
	return nil
}

func (c *ChatStreamEvent) String() string {
	if len(c._rawJSON) > 0 {
		if value, err := core.StringifyJSON(c._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(c); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", c)
}

// Defaults to `"accurate"`.
// Dictates the approach taken to generating citations as part of the RAG flow by allowing the user to specify whether they want `"accurate"` results or `"fast"` results.
type ChatStreamRequestCitationQuality string

const (
	ChatStreamRequestCitationQualityFast     ChatStreamRequestCitationQuality = "fast"
	ChatStreamRequestCitationQualityAccurate ChatStreamRequestCitationQuality = "accurate"
)

func NewChatStreamRequestCitationQualityFromString(s string) (ChatStreamRequestCitationQuality, error) {
	switch s {
	case "fast":
		return ChatStreamRequestCitationQualityFast, nil
	case "accurate":
		return ChatStreamRequestCitationQualityAccurate, nil
	}
	var t ChatStreamRequestCitationQuality
	return "", fmt.Errorf("%s is not a valid %T", s, t)
}

func (c ChatStreamRequestCitationQuality) Ptr() *ChatStreamRequestCitationQuality {
	return &c
}

// Defaults to `AUTO` when `connectors` are specified and `OFF` in all other cases.
// Dictates how the prompt will be constructed.
// With `prompt_truncation` set to "AUTO", some elements from `chat_history` and `documents` will be dropped in an attempt to construct a prompt that fits within the model's context length limit.
// With `prompt_truncation` set to "OFF", no elements will be dropped. If the sum of the inputs exceeds the model's context length limit, a `TooManyTokens` error will be returned.
type ChatStreamRequestPromptTruncation string

const (
	ChatStreamRequestPromptTruncationOff  ChatStreamRequestPromptTruncation = "OFF"
	ChatStreamRequestPromptTruncationAuto ChatStreamRequestPromptTruncation = "AUTO"
)

func NewChatStreamRequestPromptTruncationFromString(s string) (ChatStreamRequestPromptTruncation, error) {
	switch s {
	case "OFF":
		return ChatStreamRequestPromptTruncationOff, nil
	case "AUTO":
		return ChatStreamRequestPromptTruncationAuto, nil
	}
	var t ChatStreamRequestPromptTruncation
	return "", fmt.Errorf("%s is not a valid %T", s, t)
}

func (c ChatStreamRequestPromptTruncation) Ptr() *ChatStreamRequestPromptTruncation {
	return &c
}

type ChatStreamStartEvent struct {
	// Unique identifier for the generated reply. Useful for submitting feedback.
	GenerationId string `json:"generation_id"`

	_rawJSON json.RawMessage
}

func (c *ChatStreamStartEvent) UnmarshalJSON(data []byte) error {
	type unmarshaler ChatStreamStartEvent
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*c = ChatStreamStartEvent(value)
	c._rawJSON = json.RawMessage(data)
	return nil
}

func (c *ChatStreamStartEvent) String() string {
	if len(c._rawJSON) > 0 {
		if value, err := core.StringifyJSON(c._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(c); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", c)
}

type ChatTextGenerationEvent struct {
	// The next batch of text generated by the model.
	Text string `json:"text"`

	_rawJSON json.RawMessage
}

func (c *ChatTextGenerationEvent) UnmarshalJSON(data []byte) error {
	type unmarshaler ChatTextGenerationEvent
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*c = ChatTextGenerationEvent(value)
	c._rawJSON = json.RawMessage(data)
	return nil
}

func (c *ChatTextGenerationEvent) String() string {
	if len(c._rawJSON) > 0 {
		if value, err := core.StringifyJSON(c._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(c); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", c)
}

type ClassifyRequestExamplesItem struct {
	Text  *string `json:"text,omitempty"`
	Label *string `json:"label,omitempty"`

	_rawJSON json.RawMessage
}

func (c *ClassifyRequestExamplesItem) UnmarshalJSON(data []byte) error {
	type unmarshaler ClassifyRequestExamplesItem
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*c = ClassifyRequestExamplesItem(value)
	c._rawJSON = json.RawMessage(data)
	return nil
}

func (c *ClassifyRequestExamplesItem) String() string {
	if len(c._rawJSON) > 0 {
		if value, err := core.StringifyJSON(c._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(c); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", c)
}

// One of `NONE|START|END` to specify how the API will handle inputs longer than the maximum token length.
// Passing `START` will discard the start of the input. `END` will discard the end of the input. In both cases, input is discarded until the remaining input is exactly the maximum input token length for the model.
// If `NONE` is selected, when the input exceeds the maximum input token length an error will be returned.
type ClassifyRequestTruncate string

const (
	ClassifyRequestTruncateNone  ClassifyRequestTruncate = "NONE"
	ClassifyRequestTruncateStart ClassifyRequestTruncate = "START"
	ClassifyRequestTruncateEnd   ClassifyRequestTruncate = "END"
)

func NewClassifyRequestTruncateFromString(s string) (ClassifyRequestTruncate, error) {
	switch s {
	case "NONE":
		return ClassifyRequestTruncateNone, nil
	case "START":
		return ClassifyRequestTruncateStart, nil
	case "END":
		return ClassifyRequestTruncateEnd, nil
	}
	var t ClassifyRequestTruncate
	return "", fmt.Errorf("%s is not a valid %T", s, t)
}

func (c ClassifyRequestTruncate) Ptr() *ClassifyRequestTruncate {
	return &c
}

type ClassifyResponse struct {
	Id              string                                 `json:"id"`
	Classifications []*ClassifyResponseClassificationsItem `json:"classifications,omitempty"`
	Meta            *ApiMeta                               `json:"meta,omitempty"`

	_rawJSON json.RawMessage
}

func (c *ClassifyResponse) UnmarshalJSON(data []byte) error {
	type unmarshaler ClassifyResponse
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*c = ClassifyResponse(value)
	c._rawJSON = json.RawMessage(data)
	return nil
}

func (c *ClassifyResponse) String() string {
	if len(c._rawJSON) > 0 {
		if value, err := core.StringifyJSON(c._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(c); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", c)
}

type ClassifyResponseClassificationsItem struct {
	Id string `json:"id"`
	// The input text that was classified
	Input *string `json:"input,omitempty"`
	// The predicted label for the associated query (only filled for single-label models)
	Prediction *string `json:"prediction,omitempty"`
	// An array containing the predicted labels for the associated query (only filled for single-label classification)
	Predictions []string `json:"predictions,omitempty"`
	// The confidence score for the top predicted class (only filled for single-label classification)
	Confidence *float64 `json:"confidence,omitempty"`
	// An array containing the confidence scores of all the predictions in the same order
	Confidences []float64 `json:"confidences,omitempty"`
	// A map containing each label and its confidence score according to the classifier. All the confidence scores add up to 1 for single-label classification. For multi-label classification the label confidences are independent of each other, so they don't have to sum up to 1.
	Labels map[string]*ClassifyResponseClassificationsItemLabelsValue `json:"labels,omitempty"`
	// The type of classification performed
	ClassificationType ClassifyResponseClassificationsItemClassificationType `json:"classification_type,omitempty"`

	_rawJSON json.RawMessage
}

func (c *ClassifyResponseClassificationsItem) UnmarshalJSON(data []byte) error {
	type unmarshaler ClassifyResponseClassificationsItem
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*c = ClassifyResponseClassificationsItem(value)
	c._rawJSON = json.RawMessage(data)
	return nil
}

func (c *ClassifyResponseClassificationsItem) String() string {
	if len(c._rawJSON) > 0 {
		if value, err := core.StringifyJSON(c._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(c); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", c)
}

// The type of classification performed
type ClassifyResponseClassificationsItemClassificationType string

const (
	ClassifyResponseClassificationsItemClassificationTypeSingleLabel ClassifyResponseClassificationsItemClassificationType = "single-label"
	ClassifyResponseClassificationsItemClassificationTypeMultiLabel  ClassifyResponseClassificationsItemClassificationType = "multi-label"
)

func NewClassifyResponseClassificationsItemClassificationTypeFromString(s string) (ClassifyResponseClassificationsItemClassificationType, error) {
	switch s {
	case "single-label":
		return ClassifyResponseClassificationsItemClassificationTypeSingleLabel, nil
	case "multi-label":
		return ClassifyResponseClassificationsItemClassificationTypeMultiLabel, nil
	}
	var t ClassifyResponseClassificationsItemClassificationType
	return "", fmt.Errorf("%s is not a valid %T", s, t)
}

func (c ClassifyResponseClassificationsItemClassificationType) Ptr() *ClassifyResponseClassificationsItemClassificationType {
	return &c
}

type ClassifyResponseClassificationsItemLabelsValue struct {
	Confidence *float64 `json:"confidence,omitempty"`

	_rawJSON json.RawMessage
}

func (c *ClassifyResponseClassificationsItemLabelsValue) UnmarshalJSON(data []byte) error {
	type unmarshaler ClassifyResponseClassificationsItemLabelsValue
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*c = ClassifyResponseClassificationsItemLabelsValue(value)
	c._rawJSON = json.RawMessage(data)
	return nil
}

func (c *ClassifyResponseClassificationsItemLabelsValue) String() string {
	if len(c._rawJSON) > 0 {
		if value, err := core.StringifyJSON(c._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(c); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", c)
}

type Cluster struct {
	Id             *string  `json:"id,omitempty"`
	Keywords       []string `json:"keywords,omitempty"`
	Description    *string  `json:"description,omitempty"`
	Size           *int     `json:"size,omitempty"`
	SampleElements []string `json:"sample_elements,omitempty"`

	_rawJSON json.RawMessage
}

func (c *Cluster) UnmarshalJSON(data []byte) error {
	type unmarshaler Cluster
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*c = Cluster(value)
	c._rawJSON = json.RawMessage(data)
	return nil
}

func (c *Cluster) String() string {
	if len(c._rawJSON) > 0 {
		if value, err := core.StringifyJSON(c._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(c); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", c)
}

// Response for creating a cluster job.
type CreateClusterJobResponse struct {
	JobId string `json:"job_id"`

	_rawJSON json.RawMessage
}

func (c *CreateClusterJobResponse) UnmarshalJSON(data []byte) error {
	type unmarshaler CreateClusterJobResponse
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*c = CreateClusterJobResponse(value)
	c._rawJSON = json.RawMessage(data)
	return nil
}

func (c *CreateClusterJobResponse) String() string {
	if len(c._rawJSON) > 0 {
		if value, err := core.StringifyJSON(c._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(c); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", c)
}

type Dataset struct {
	// The dataset ID
	Id *string `json:"id,omitempty"`
	// The name of the dataset
	Name *string `json:"name,omitempty"`
	// The creation date
	CreatedAt *time.Time `json:"createdAt,omitempty"`
	// The last update date
	UpdatedAt *time.Time `json:"updatedAt,omitempty"`
	// The type of the dataset
	DatasetType *string `json:"datasetType,omitempty"`
	// The validation status of the dataset
	ValidationStatus *string `json:"validationStatus,omitempty"`
	// errors found during validation
	ValidationError *string `json:"validationError,omitempty"`
	// the avro schema of the dataset
	Schema         *string  `json:"schema,omitempty"`
	RequiredFields []string `json:"requiredFields,omitempty"`
	PreserveFields []string `json:"preserveFields,omitempty"`
	// the underlying files that make up the dataset
	DatasetParts []*DatasetPart `json:"datasetParts,omitempty"`
	// warnings found during validation
	ValidationWarnings []string `json:"validationWarnings,omitempty"`

	_rawJSON json.RawMessage
}

func (d *Dataset) UnmarshalJSON(data []byte) error {
	type unmarshaler Dataset
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*d = Dataset(value)
	d._rawJSON = json.RawMessage(data)
	return nil
}

func (d *Dataset) String() string {
	if len(d._rawJSON) > 0 {
		if value, err := core.StringifyJSON(d._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(d); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", d)
}

type DatasetPart struct {
	// The dataset part ID
	Id *string `json:"id,omitempty"`
	// The name of the dataset part
	Name *string `json:"name,omitempty"`
	// The download url of the file
	Url *string `json:"url,omitempty"`
	// The index of the file
	Index *string `json:"index,omitempty"`
	// The size of the file in bytes
	SizeBytes *string `json:"sizeBytes,omitempty"`
	// The number of rows in the file
	NumRows *string `json:"numRows,omitempty"`
	// The download url of the original file
	OriginalUrl *string `json:"originalUrl,omitempty"`

	_rawJSON json.RawMessage
}

func (d *DatasetPart) UnmarshalJSON(data []byte) error {
	type unmarshaler DatasetPart
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*d = DatasetPart(value)
	d._rawJSON = json.RawMessage(data)
	return nil
}

func (d *DatasetPart) String() string {
	if len(d._rawJSON) > 0 {
		if value, err := core.StringifyJSON(d._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(d); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", d)
}

type DetectLanguageResponse struct {
	// List of languages, one per input text
	Results []*DetectLanguageResponseResultsItem `json:"results,omitempty"`
	Meta    *ApiMeta                             `json:"meta,omitempty"`

	_rawJSON json.RawMessage
}

func (d *DetectLanguageResponse) UnmarshalJSON(data []byte) error {
	type unmarshaler DetectLanguageResponse
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*d = DetectLanguageResponse(value)
	d._rawJSON = json.RawMessage(data)
	return nil
}

func (d *DetectLanguageResponse) String() string {
	if len(d._rawJSON) > 0 {
		if value, err := core.StringifyJSON(d._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(d); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", d)
}

type DetectLanguageResponseResultsItem struct {
	LanguageName *string `json:"language_name,omitempty"`
	LanguageCode *string `json:"language_code,omitempty"`

	_rawJSON json.RawMessage
}

func (d *DetectLanguageResponseResultsItem) UnmarshalJSON(data []byte) error {
	type unmarshaler DetectLanguageResponseResultsItem
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*d = DetectLanguageResponseResultsItem(value)
	d._rawJSON = json.RawMessage(data)
	return nil
}

func (d *DetectLanguageResponseResultsItem) String() string {
	if len(d._rawJSON) > 0 {
		if value, err := core.StringifyJSON(d._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(d); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", d)
}

type DetokenizeResponse struct {
	// A string representing the list of tokens.
	Text string   `json:"text"`
	Meta *ApiMeta `json:"meta,omitempty"`

	_rawJSON json.RawMessage
}

func (d *DetokenizeResponse) UnmarshalJSON(data []byte) error {
	type unmarshaler DetokenizeResponse
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*d = DetokenizeResponse(value)
	d._rawJSON = json.RawMessage(data)
	return nil
}

func (d *DetokenizeResponse) String() string {
	if len(d._rawJSON) > 0 {
		if value, err := core.StringifyJSON(d._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(d); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", d)
}

// One of `NONE|START|END` to specify how the API will handle inputs longer than the maximum token length.
//
// Passing `START` will discard the start of the input. `END` will discard the end of the input. In both cases, input is discarded until the remaining input is exactly the maximum input token length for the model.
//
// If `NONE` is selected, when the input exceeds the maximum input token length an error will be returned.
type EmbedRequestTruncate string

const (
	EmbedRequestTruncateNone  EmbedRequestTruncate = "NONE"
	EmbedRequestTruncateStart EmbedRequestTruncate = "START"
	EmbedRequestTruncateEnd   EmbedRequestTruncate = "END"
)

func NewEmbedRequestTruncateFromString(s string) (EmbedRequestTruncate, error) {
	switch s {
	case "NONE":
		return EmbedRequestTruncateNone, nil
	case "START":
		return EmbedRequestTruncateStart, nil
	case "END":
		return EmbedRequestTruncateEnd, nil
	}
	var t EmbedRequestTruncate
	return "", fmt.Errorf("%s is not a valid %T", s, t)
}

func (e EmbedRequestTruncate) Ptr() *EmbedRequestTruncate {
	return &e
}

type EmbedResponse struct {
	Id string `json:"id"`
	// An array of embeddings, where each embedding is an array of floats. The length of the `embeddings` array will be the same as the length of the original `texts` array.
	Embeddings [][]float64 `json:"embeddings,omitempty"`
	// The text entries for which embeddings were returned.
	Texts []string `json:"texts,omitempty"`
	Meta  *ApiMeta `json:"meta,omitempty"`

	_rawJSON json.RawMessage
}

func (e *EmbedResponse) UnmarshalJSON(data []byte) error {
	type unmarshaler EmbedResponse
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*e = EmbedResponse(value)
	e._rawJSON = json.RawMessage(data)
	return nil
}

func (e *EmbedResponse) String() string {
	if len(e._rawJSON) > 0 {
		if value, err := core.StringifyJSON(e._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(e); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", e)
}

type FinishReason string

const (
	FinishReasonComplete   FinishReason = "COMPLETE"
	FinishReasonError      FinishReason = "ERROR"
	FinishReasonErrorToxic FinishReason = "ERROR_TOXIC"
	FinishReasonErrorLimit FinishReason = "ERROR_LIMIT"
	FinishReasonUserCancel FinishReason = "USER_CANCEL"
	FinishReasonMaxTokens  FinishReason = "MAX_TOKENS"
)

func NewFinishReasonFromString(s string) (FinishReason, error) {
	switch s {
	case "COMPLETE":
		return FinishReasonComplete, nil
	case "ERROR":
		return FinishReasonError, nil
	case "ERROR_TOXIC":
		return FinishReasonErrorToxic, nil
	case "ERROR_LIMIT":
		return FinishReasonErrorLimit, nil
	case "USER_CANCEL":
		return FinishReasonUserCancel, nil
	case "MAX_TOKENS":
		return FinishReasonMaxTokens, nil
	}
	var t FinishReason
	return "", fmt.Errorf("%s is not a valid %T", s, t)
}

func (f FinishReason) Ptr() *FinishReason {
	return &f
}

// One of `GENERATION|ALL|NONE` to specify how and if the token likelihoods are returned with the response. Defaults to `NONE`.
//
// If `GENERATION` is selected, the token likelihoods will only be provided for generated text.
//
// If `ALL` is selected, the token likelihoods will be provided both for the prompt and the generated text.
type GenerateRequestReturnLikelihoods string

const (
	GenerateRequestReturnLikelihoodsGeneration GenerateRequestReturnLikelihoods = "GENERATION"
	GenerateRequestReturnLikelihoodsAll        GenerateRequestReturnLikelihoods = "ALL"
	GenerateRequestReturnLikelihoodsNone       GenerateRequestReturnLikelihoods = "NONE"
)

func NewGenerateRequestReturnLikelihoodsFromString(s string) (GenerateRequestReturnLikelihoods, error) {
	switch s {
	case "GENERATION":
		return GenerateRequestReturnLikelihoodsGeneration, nil
	case "ALL":
		return GenerateRequestReturnLikelihoodsAll, nil
	case "NONE":
		return GenerateRequestReturnLikelihoodsNone, nil
	}
	var t GenerateRequestReturnLikelihoods
	return "", fmt.Errorf("%s is not a valid %T", s, t)
}

func (g GenerateRequestReturnLikelihoods) Ptr() *GenerateRequestReturnLikelihoods {
	return &g
}

// One of `NONE|START|END` to specify how the API will handle inputs longer than the maximum token length.
//
// Passing `START` will discard the start of the input. `END` will discard the end of the input. In both cases, input is discarded until the remaining input is exactly the maximum input token length for the model.
//
// If `NONE` is selected, when the input exceeds the maximum input token length an error will be returned.
type GenerateRequestTruncate string

const (
	GenerateRequestTruncateNone  GenerateRequestTruncate = "NONE"
	GenerateRequestTruncateStart GenerateRequestTruncate = "START"
	GenerateRequestTruncateEnd   GenerateRequestTruncate = "END"
)

func NewGenerateRequestTruncateFromString(s string) (GenerateRequestTruncate, error) {
	switch s {
	case "NONE":
		return GenerateRequestTruncateNone, nil
	case "START":
		return GenerateRequestTruncateStart, nil
	case "END":
		return GenerateRequestTruncateEnd, nil
	}
	var t GenerateRequestTruncate
	return "", fmt.Errorf("%s is not a valid %T", s, t)
}

func (g GenerateRequestTruncate) Ptr() *GenerateRequestTruncate {
	return &g
}

type Generation struct {
	Id string `json:"id"`
	// Prompt used for generations.
	Prompt *string `json:"prompt,omitempty"`
	// List of generated results
	Generations []*SingleGeneration `json:"generations,omitempty"`
	Meta        *ApiMeta            `json:"meta,omitempty"`

	_rawJSON json.RawMessage
}

func (g *Generation) UnmarshalJSON(data []byte) error {
	type unmarshaler Generation
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*g = Generation(value)
	g._rawJSON = json.RawMessage(data)
	return nil
}

func (g *Generation) String() string {
	if len(g._rawJSON) > 0 {
		if value, err := core.StringifyJSON(g._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(g); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", g)
}

type GenerationFinalResponse struct {
	IsFinished   bool                             `json:"is_finished"`
	FinishReason *FinishReason                    `json:"finish_reason,omitempty"`
	Response     *GenerationFinalResponseResponse `json:"response,omitempty"`

	_rawJSON json.RawMessage
}

func (g *GenerationFinalResponse) UnmarshalJSON(data []byte) error {
	type unmarshaler GenerationFinalResponse
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*g = GenerationFinalResponse(value)
	g._rawJSON = json.RawMessage(data)
	return nil
}

func (g *GenerationFinalResponse) String() string {
	if len(g._rawJSON) > 0 {
		if value, err := core.StringifyJSON(g._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(g); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", g)
}

type GenerationFinalResponseResponse struct {
	Id          string                    `json:"id"`
	Generations *SingleGenerationInStream `json:"generations,omitempty"`

	_rawJSON json.RawMessage
}

func (g *GenerationFinalResponseResponse) UnmarshalJSON(data []byte) error {
	type unmarshaler GenerationFinalResponseResponse
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*g = GenerationFinalResponseResponse(value)
	g._rawJSON = json.RawMessage(data)
	return nil
}

func (g *GenerationFinalResponseResponse) String() string {
	if len(g._rawJSON) > 0 {
		if value, err := core.StringifyJSON(g._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(g); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", g)
}

type GenerationStream struct {
	// A segment of text of the generation.
	Text string `json:"text"`
	// Refers to the nth generation. Only present when `num_generations` is greater than zero, and only when text responses are being streamed.
	Index      *int `json:"index,omitempty"`
	IsFinished bool `json:"is_finished"`

	_rawJSON json.RawMessage
}

func (g *GenerationStream) UnmarshalJSON(data []byte) error {
	type unmarshaler GenerationStream
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*g = GenerationStream(value)
	g._rawJSON = json.RawMessage(data)
	return nil
}

func (g *GenerationStream) String() string {
	if len(g._rawJSON) > 0 {
		if value, err := core.StringifyJSON(g._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(g); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", g)
}

// Response for getting a cluster job.
type GetClusterJobResponse struct {
	JobId string `json:"job_id"`
	// Time of job creation in RFC3339 format
	CreatedAt *time.Time `json:"created_at,omitempty"`
	// The input file URL used for the job
	EmbeddingsUrl *string `json:"embeddings_url,omitempty"`
	// The input dataset ID used for the job
	InputDatasetId *string `json:"input_dataset_id,omitempty"`
	// The parameter used in the job creation. Please refer to the job creation endpoint for more details
	MinClusterSize *int `json:"min_cluster_size,omitempty"`
	// The parameter used in the job creation. Please refer to the job creation endpoint for more details
	NNeighbors *int `json:"n_neighbors,omitempty"`
	// The parameter used in the job creation. Please refer to the job creation endpoint for more details
	IsDeterministic *bool                        `json:"is_deterministic,omitempty"`
	Status          *GetClusterJobResponseStatus `json:"status,omitempty"`
	// A boolean indicating whether the job is in a final state, whether completed or failed
	IsFinalState *bool `json:"is_final_state,omitempty"`
	// The output file URL for the clusters (signed url that expires)
	OutputClustersUrl *string `json:"output_clusters_url,omitempty"`
	// The output file URL for the outliers (signed url that expires)
	OutputOutliersUrl *string `json:"output_outliers_url,omitempty"`
	// The list of cluster summaries for the job
	Clusters []*Cluster `json:"clusters,omitempty"`
	Error    *string    `json:"error,omitempty"`
	Meta     *ApiMeta   `json:"meta,omitempty"`

	_rawJSON json.RawMessage
}

func (g *GetClusterJobResponse) UnmarshalJSON(data []byte) error {
	type unmarshaler GetClusterJobResponse
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*g = GetClusterJobResponse(value)
	g._rawJSON = json.RawMessage(data)
	return nil
}

func (g *GetClusterJobResponse) String() string {
	if len(g._rawJSON) > 0 {
		if value, err := core.StringifyJSON(g._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(g); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", g)
}

type GetClusterJobResponseStatus string

const (
	GetClusterJobResponseStatusUnknown    GetClusterJobResponseStatus = "unknown"
	GetClusterJobResponseStatusProcessing GetClusterJobResponseStatus = "processing"
	GetClusterJobResponseStatusFailed     GetClusterJobResponseStatus = "failed"
	GetClusterJobResponseStatusComplete   GetClusterJobResponseStatus = "complete"
	GetClusterJobResponseStatusQueued     GetClusterJobResponseStatus = "queued"
)

func NewGetClusterJobResponseStatusFromString(s string) (GetClusterJobResponseStatus, error) {
	switch s {
	case "unknown":
		return GetClusterJobResponseStatusUnknown, nil
	case "processing":
		return GetClusterJobResponseStatusProcessing, nil
	case "failed":
		return GetClusterJobResponseStatusFailed, nil
	case "complete":
		return GetClusterJobResponseStatusComplete, nil
	case "queued":
		return GetClusterJobResponseStatusQueued, nil
	}
	var t GetClusterJobResponseStatus
	return "", fmt.Errorf("%s is not a valid %T", s, t)
}

func (g GetClusterJobResponseStatus) Ptr() *GetClusterJobResponseStatus {
	return &g
}

type ListClusterJobsResponse struct {
	Jobs       []*GetClusterJobResponse `json:"jobs,omitempty"`
	TotalCount *int                     `json:"total_count,omitempty"`
	Meta       *ApiMeta                 `json:"meta,omitempty"`

	_rawJSON json.RawMessage
}

func (l *ListClusterJobsResponse) UnmarshalJSON(data []byte) error {
	type unmarshaler ListClusterJobsResponse
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*l = ListClusterJobsResponse(value)
	l._rawJSON = json.RawMessage(data)
	return nil
}

func (l *ListClusterJobsResponse) String() string {
	if len(l._rawJSON) > 0 {
		if value, err := core.StringifyJSON(l._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(l); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", l)
}

type LogLikelihoodResponse struct {
	Id string `json:"id"`
	// Probabilities for tokens in the request prompt
	PromptTokens []interface{} `json:"prompt_tokens,omitempty"`
	// Probabilities for tokens in the request completion
	CompletionTokens []interface{} `json:"completion_tokens,omitempty"`
	// Probabilities for tokens in the request raw_prompt
	RawPromptTokens []interface{} `json:"raw_prompt_tokens,omitempty"`
	Meta            *ApiMeta      `json:"meta,omitempty"`

	_rawJSON json.RawMessage
}

func (l *LogLikelihoodResponse) UnmarshalJSON(data []byte) error {
	type unmarshaler LogLikelihoodResponse
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*l = LogLikelihoodResponse(value)
	l._rawJSON = json.RawMessage(data)
	return nil
}

func (l *LogLikelihoodResponse) String() string {
	if len(l._rawJSON) > 0 {
		if value, err := core.StringifyJSON(l._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(l); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", l)
}

type NonStreamedChatResponse struct {
	// Contents of the reply generated by the model.
	Text string `json:"text"`
	// Unique identifier for the generated reply. Useful for submitting feedback.
	GenerationId string `json:"generation_id"`
	// Inline citations for the generated reply.
	Citations []*ChatCitation `json:"citations,omitempty"`
	// Documents seen by the model when generating the reply.
	Documents []ChatDocument `json:"documents,omitempty"`
	// Generated search queries, meant to be used as part of the RAG flow.
	SearchQueries []*ChatSearchQuery `json:"search_queries,omitempty"`
	// Documents retrieved from each of the conducted searches.
	SearchResults []*ChatSearchResult `json:"search_results,omitempty"`

	_rawJSON json.RawMessage
}

func (n *NonStreamedChatResponse) UnmarshalJSON(data []byte) error {
	type unmarshaler NonStreamedChatResponse
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*n = NonStreamedChatResponse(value)
	n._rawJSON = json.RawMessage(data)
	return nil
}

func (n *NonStreamedChatResponse) String() string {
	if len(n._rawJSON) > 0 {
		if value, err := core.StringifyJSON(n._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(n); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", n)
}

type RerankRequestDocumentsItem struct {
	typeName                       string
	String                         string
	RerankRequestDocumentsItemText *RerankRequestDocumentsItemText
}

func NewRerankRequestDocumentsItemFromString(value string) *RerankRequestDocumentsItem {
	return &RerankRequestDocumentsItem{typeName: "string", String: value}
}

func NewRerankRequestDocumentsItemFromRerankRequestDocumentsItemText(value *RerankRequestDocumentsItemText) *RerankRequestDocumentsItem {
	return &RerankRequestDocumentsItem{typeName: "rerankRequestDocumentsItemText", RerankRequestDocumentsItemText: value}
}

func (r *RerankRequestDocumentsItem) UnmarshalJSON(data []byte) error {
	var valueString string
	if err := json.Unmarshal(data, &valueString); err == nil {
		r.typeName = "string"
		r.String = valueString
		return nil
	}
	valueRerankRequestDocumentsItemText := new(RerankRequestDocumentsItemText)
	if err := json.Unmarshal(data, &valueRerankRequestDocumentsItemText); err == nil {
		r.typeName = "rerankRequestDocumentsItemText"
		r.RerankRequestDocumentsItemText = valueRerankRequestDocumentsItemText
		return nil
	}
	return fmt.Errorf("%s cannot be deserialized as a %T", data, r)
}

func (r RerankRequestDocumentsItem) MarshalJSON() ([]byte, error) {
	switch r.typeName {
	default:
		return nil, fmt.Errorf("invalid type %s in %T", r.typeName, r)
	case "string":
		return json.Marshal(r.String)
	case "rerankRequestDocumentsItemText":
		return json.Marshal(r.RerankRequestDocumentsItemText)
	}
}

type RerankRequestDocumentsItemVisitor interface {
	VisitString(string) error
	VisitRerankRequestDocumentsItemText(*RerankRequestDocumentsItemText) error
}

func (r *RerankRequestDocumentsItem) Accept(visitor RerankRequestDocumentsItemVisitor) error {
	switch r.typeName {
	default:
		return fmt.Errorf("invalid type %s in %T", r.typeName, r)
	case "string":
		return visitor.VisitString(r.String)
	case "rerankRequestDocumentsItemText":
		return visitor.VisitRerankRequestDocumentsItemText(r.RerankRequestDocumentsItemText)
	}
}

type RerankRequestDocumentsItemText struct {
	// The text of the document to rerank.
	Text string `json:"text"`

	_rawJSON json.RawMessage
}

func (r *RerankRequestDocumentsItemText) UnmarshalJSON(data []byte) error {
	type unmarshaler RerankRequestDocumentsItemText
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*r = RerankRequestDocumentsItemText(value)
	r._rawJSON = json.RawMessage(data)
	return nil
}

func (r *RerankRequestDocumentsItemText) String() string {
	if len(r._rawJSON) > 0 {
		if value, err := core.StringifyJSON(r._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(r); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", r)
}

type RerankResponse struct {
	Id *string `json:"id,omitempty"`
	// An ordered list of ranked documents
	Results []*RerankResponseResultsItem `json:"results,omitempty"`
	Meta    *ApiMeta                     `json:"meta,omitempty"`

	_rawJSON json.RawMessage
}

func (r *RerankResponse) UnmarshalJSON(data []byte) error {
	type unmarshaler RerankResponse
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*r = RerankResponse(value)
	r._rawJSON = json.RawMessage(data)
	return nil
}

func (r *RerankResponse) String() string {
	if len(r._rawJSON) > 0 {
		if value, err := core.StringifyJSON(r._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(r); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", r)
}

type RerankResponseResultsItem struct {
	// The doc object which was ranked
	Document *RerankResponseResultsItemDocument `json:"document,omitempty"`
	// The index of the input document
	Index int `json:"index"`
	// A relevance score assigned to the ranking
	RelevanceScore float64 `json:"relevance_score"`

	_rawJSON json.RawMessage
}

func (r *RerankResponseResultsItem) UnmarshalJSON(data []byte) error {
	type unmarshaler RerankResponseResultsItem
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*r = RerankResponseResultsItem(value)
	r._rawJSON = json.RawMessage(data)
	return nil
}

func (r *RerankResponseResultsItem) String() string {
	if len(r._rawJSON) > 0 {
		if value, err := core.StringifyJSON(r._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(r); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", r)
}

// The doc object which was ranked
type RerankResponseResultsItemDocument struct {
	// The text of the document to rerank
	Text string `json:"text"`

	_rawJSON json.RawMessage
}

func (r *RerankResponseResultsItemDocument) UnmarshalJSON(data []byte) error {
	type unmarshaler RerankResponseResultsItemDocument
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*r = RerankResponseResultsItemDocument(value)
	r._rawJSON = json.RawMessage(data)
	return nil
}

func (r *RerankResponseResultsItemDocument) String() string {
	if len(r._rawJSON) > 0 {
		if value, err := core.StringifyJSON(r._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(r); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", r)
}

type SearchQueriesOnlyResponse struct {
	// Generated search queries, meant to be used as part of the RAG flow.
	SearchQueries []*ChatSearchQuery `json:"search_queries,omitempty"`

	_rawJSON json.RawMessage
}

func (s *SearchQueriesOnlyResponse) UnmarshalJSON(data []byte) error {
	type unmarshaler SearchQueriesOnlyResponse
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*s = SearchQueriesOnlyResponse(value)
	s._rawJSON = json.RawMessage(data)
	return nil
}

func (s *SearchQueriesOnlyResponse) String() string {
	if len(s._rawJSON) > 0 {
		if value, err := core.StringifyJSON(s._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(s); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", s)
}

type SingleGeneration struct {
	Id   string `json:"id"`
	Text string `json:"text"`
	// Refers to the nth generation. Only present when `num_generations` is greater than zero.
	Index      *int     `json:"index,omitempty"`
	Likelihood *float64 `json:"likelihood,omitempty"`
	// Only returned if `return_likelihoods` is set to `GENERATION` or `ALL`. The likelihood refers to the average log-likelihood of the entire specified string, which is useful for [evaluating the performance of your model](likelihood-eval), especially if you've created a [custom model](/docs/training-custom-models). Individual token likelihoods provide the log-likelihood of each token. The first token will not have a likelihood.
	TokenLikelihoods []*SingleGenerationTokenLikelihoodsItem `json:"token_likelihoods,omitempty"`

	_rawJSON json.RawMessage
}

func (s *SingleGeneration) UnmarshalJSON(data []byte) error {
	type unmarshaler SingleGeneration
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*s = SingleGeneration(value)
	s._rawJSON = json.RawMessage(data)
	return nil
}

func (s *SingleGeneration) String() string {
	if len(s._rawJSON) > 0 {
		if value, err := core.StringifyJSON(s._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(s); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", s)
}

type SingleGenerationInStream struct {
	Id string `json:"id"`
	// Full text of the generation.
	Text string `json:"text"`
	// Refers to the nth generation. Only present when `num_generations` is greater than zero.
	Index        *int         `json:"index,omitempty"`
	FinishReason FinishReason `json:"finish_reason,omitempty"`

	_rawJSON json.RawMessage
}

func (s *SingleGenerationInStream) UnmarshalJSON(data []byte) error {
	type unmarshaler SingleGenerationInStream
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*s = SingleGenerationInStream(value)
	s._rawJSON = json.RawMessage(data)
	return nil
}

func (s *SingleGenerationInStream) String() string {
	if len(s._rawJSON) > 0 {
		if value, err := core.StringifyJSON(s._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(s); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", s)
}

type SingleGenerationTokenLikelihoodsItem struct {
	Token      string  `json:"token"`
	Likelihood float64 `json:"likelihood"`

	_rawJSON json.RawMessage
}

func (s *SingleGenerationTokenLikelihoodsItem) UnmarshalJSON(data []byte) error {
	type unmarshaler SingleGenerationTokenLikelihoodsItem
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*s = SingleGenerationTokenLikelihoodsItem(value)
	s._rawJSON = json.RawMessage(data)
	return nil
}

func (s *SingleGenerationTokenLikelihoodsItem) String() string {
	if len(s._rawJSON) > 0 {
		if value, err := core.StringifyJSON(s._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(s); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", s)
}

// StreamedChatResponse is returned in streaming mode (specified with `stream=True` in the request).
type StreamedChatResponse struct {
	EventType               string
	StreamStart             *ChatStreamStartEvent
	SearchQueriesGeneration *ChatSearchQueriesGenerationEvent
	SearchResults           *ChatSearchResultsEvent
	TextGeneration          *ChatTextGenerationEvent
	CitationGeneration      *ChatCitationGenerationEvent
	StreamEnd               *ChatStreamEndEvent
}

func NewStreamedChatResponseFromStreamStart(value *ChatStreamStartEvent) *StreamedChatResponse {
	return &StreamedChatResponse{EventType: "stream-start", StreamStart: value}
}

func NewStreamedChatResponseFromSearchQueriesGeneration(value *ChatSearchQueriesGenerationEvent) *StreamedChatResponse {
	return &StreamedChatResponse{EventType: "search-queries-generation", SearchQueriesGeneration: value}
}

func NewStreamedChatResponseFromSearchResults(value *ChatSearchResultsEvent) *StreamedChatResponse {
	return &StreamedChatResponse{EventType: "search-results", SearchResults: value}
}

func NewStreamedChatResponseFromTextGeneration(value *ChatTextGenerationEvent) *StreamedChatResponse {
	return &StreamedChatResponse{EventType: "text-generation", TextGeneration: value}
}

func NewStreamedChatResponseFromCitationGeneration(value *ChatCitationGenerationEvent) *StreamedChatResponse {
	return &StreamedChatResponse{EventType: "citation-generation", CitationGeneration: value}
}

func NewStreamedChatResponseFromStreamEnd(value *ChatStreamEndEvent) *StreamedChatResponse {
	return &StreamedChatResponse{EventType: "stream-end", StreamEnd: value}
}

func (s *StreamedChatResponse) UnmarshalJSON(data []byte) error {
	var unmarshaler struct {
		EventType string `json:"event_type"`
	}
	if err := json.Unmarshal(data, &unmarshaler); err != nil {
		return err
	}
	s.EventType = unmarshaler.EventType
	switch unmarshaler.EventType {
	case "stream-start":
		value := new(ChatStreamStartEvent)
		if err := json.Unmarshal(data, &value); err != nil {
			return err
		}
		s.StreamStart = value
	case "search-queries-generation":
		value := new(ChatSearchQueriesGenerationEvent)
		if err := json.Unmarshal(data, &value); err != nil {
			return err
		}
		s.SearchQueriesGeneration = value
	case "search-results":
		value := new(ChatSearchResultsEvent)
		if err := json.Unmarshal(data, &value); err != nil {
			return err
		}
		s.SearchResults = value
	case "text-generation":
		value := new(ChatTextGenerationEvent)
		if err := json.Unmarshal(data, &value); err != nil {
			return err
		}
		s.TextGeneration = value
	case "citation-generation":
		value := new(ChatCitationGenerationEvent)
		if err := json.Unmarshal(data, &value); err != nil {
			return err
		}
		s.CitationGeneration = value
	case "stream-end":
		value := new(ChatStreamEndEvent)
		if err := json.Unmarshal(data, &value); err != nil {
			return err
		}
		s.StreamEnd = value
	}
	return nil
}

func (s StreamedChatResponse) MarshalJSON() ([]byte, error) {
	switch s.EventType {
	default:
		return nil, fmt.Errorf("invalid type %s in %T", s.EventType, s)
	case "stream-start":
		var marshaler = struct {
			EventType string `json:"event_type"`
			*ChatStreamStartEvent
		}{
			EventType:            s.EventType,
			ChatStreamStartEvent: s.StreamStart,
		}
		return json.Marshal(marshaler)
	case "search-queries-generation":
		var marshaler = struct {
			EventType string `json:"event_type"`
			*ChatSearchQueriesGenerationEvent
		}{
			EventType:                        s.EventType,
			ChatSearchQueriesGenerationEvent: s.SearchQueriesGeneration,
		}
		return json.Marshal(marshaler)
	case "search-results":
		var marshaler = struct {
			EventType string `json:"event_type"`
			*ChatSearchResultsEvent
		}{
			EventType:              s.EventType,
			ChatSearchResultsEvent: s.SearchResults,
		}
		return json.Marshal(marshaler)
	case "text-generation":
		var marshaler = struct {
			EventType string `json:"event_type"`
			*ChatTextGenerationEvent
		}{
			EventType:               s.EventType,
			ChatTextGenerationEvent: s.TextGeneration,
		}
		return json.Marshal(marshaler)
	case "citation-generation":
		var marshaler = struct {
			EventType string `json:"event_type"`
			*ChatCitationGenerationEvent
		}{
			EventType:                   s.EventType,
			ChatCitationGenerationEvent: s.CitationGeneration,
		}
		return json.Marshal(marshaler)
	case "stream-end":
		var marshaler = struct {
			EventType string `json:"event_type"`
			*ChatStreamEndEvent
		}{
			EventType:          s.EventType,
			ChatStreamEndEvent: s.StreamEnd,
		}
		return json.Marshal(marshaler)
	}
}

type StreamedChatResponseVisitor interface {
	VisitStreamStart(*ChatStreamStartEvent) error
	VisitSearchQueriesGeneration(*ChatSearchQueriesGenerationEvent) error
	VisitSearchResults(*ChatSearchResultsEvent) error
	VisitTextGeneration(*ChatTextGenerationEvent) error
	VisitCitationGeneration(*ChatCitationGenerationEvent) error
	VisitStreamEnd(*ChatStreamEndEvent) error
}

func (s *StreamedChatResponse) Accept(visitor StreamedChatResponseVisitor) error {
	switch s.EventType {
	default:
		return fmt.Errorf("invalid type %s in %T", s.EventType, s)
	case "stream-start":
		return visitor.VisitStreamStart(s.StreamStart)
	case "search-queries-generation":
		return visitor.VisitSearchQueriesGeneration(s.SearchQueriesGeneration)
	case "search-results":
		return visitor.VisitSearchResults(s.SearchResults)
	case "text-generation":
		return visitor.VisitTextGeneration(s.TextGeneration)
	case "citation-generation":
		return visitor.VisitCitationGeneration(s.CitationGeneration)
	case "stream-end":
		return visitor.VisitStreamEnd(s.StreamEnd)
	}
}

// Response in content type stream when `stream` is `true` in the request parameters. Generation tokens are streamed with the GenerationStream response. The final response is of type GenerationFinalResponse.
type StreamedGeneration = []*StreamedGenerationItem

type StreamedGenerationItem struct {
	typeName                string
	GenerationStream        *GenerationStream
	GenerationFinalResponse *GenerationFinalResponse
}

func NewStreamedGenerationItemFromGenerationStream(value *GenerationStream) *StreamedGenerationItem {
	return &StreamedGenerationItem{typeName: "generationStream", GenerationStream: value}
}

func NewStreamedGenerationItemFromGenerationFinalResponse(value *GenerationFinalResponse) *StreamedGenerationItem {
	return &StreamedGenerationItem{typeName: "generationFinalResponse", GenerationFinalResponse: value}
}

func (s *StreamedGenerationItem) UnmarshalJSON(data []byte) error {
	valueGenerationStream := new(GenerationStream)
	if err := json.Unmarshal(data, &valueGenerationStream); err == nil {
		s.typeName = "generationStream"
		s.GenerationStream = valueGenerationStream
		return nil
	}
	valueGenerationFinalResponse := new(GenerationFinalResponse)
	if err := json.Unmarshal(data, &valueGenerationFinalResponse); err == nil {
		s.typeName = "generationFinalResponse"
		s.GenerationFinalResponse = valueGenerationFinalResponse
		return nil
	}
	return fmt.Errorf("%s cannot be deserialized as a %T", data, s)
}

func (s StreamedGenerationItem) MarshalJSON() ([]byte, error) {
	switch s.typeName {
	default:
		return nil, fmt.Errorf("invalid type %s in %T", s.typeName, s)
	case "generationStream":
		return json.Marshal(s.GenerationStream)
	case "generationFinalResponse":
		return json.Marshal(s.GenerationFinalResponse)
	}
}

type StreamedGenerationItemVisitor interface {
	VisitGenerationStream(*GenerationStream) error
	VisitGenerationFinalResponse(*GenerationFinalResponse) error
}

func (s *StreamedGenerationItem) Accept(visitor StreamedGenerationItemVisitor) error {
	switch s.typeName {
	default:
		return fmt.Errorf("invalid type %s in %T", s.typeName, s)
	case "generationStream":
		return visitor.VisitGenerationStream(s.GenerationStream)
	case "generationFinalResponse":
		return visitor.VisitGenerationFinalResponse(s.GenerationFinalResponse)
	}
}

// One of `low`, `medium`, `high`, or `auto`, defaults to `auto`. Controls how close to the original text the summary is. `high` extractiveness summaries will lean towards reusing sentences verbatim, while `low` extractiveness summaries will tend to paraphrase more. If `auto` is selected, the best option will be picked based on the input text.
type SummarizeRequestExtractiveness string

const (
	SummarizeRequestExtractivenessLow    SummarizeRequestExtractiveness = "low"
	SummarizeRequestExtractivenessMedium SummarizeRequestExtractiveness = "medium"
	SummarizeRequestExtractivenessHigh   SummarizeRequestExtractiveness = "high"
)

func NewSummarizeRequestExtractivenessFromString(s string) (SummarizeRequestExtractiveness, error) {
	switch s {
	case "low":
		return SummarizeRequestExtractivenessLow, nil
	case "medium":
		return SummarizeRequestExtractivenessMedium, nil
	case "high":
		return SummarizeRequestExtractivenessHigh, nil
	}
	var t SummarizeRequestExtractiveness
	return "", fmt.Errorf("%s is not a valid %T", s, t)
}

func (s SummarizeRequestExtractiveness) Ptr() *SummarizeRequestExtractiveness {
	return &s
}

// One of `paragraph`, `bullets`, or `auto`, defaults to `auto`. Indicates the style in which the summary will be delivered - in a free form paragraph or in bullet points. If `auto` is selected, the best option will be picked based on the input text.
type SummarizeRequestFormat string

const (
	SummarizeRequestFormatParagraph SummarizeRequestFormat = "paragraph"
	SummarizeRequestFormatBullets   SummarizeRequestFormat = "bullets"
)

func NewSummarizeRequestFormatFromString(s string) (SummarizeRequestFormat, error) {
	switch s {
	case "paragraph":
		return SummarizeRequestFormatParagraph, nil
	case "bullets":
		return SummarizeRequestFormatBullets, nil
	}
	var t SummarizeRequestFormat
	return "", fmt.Errorf("%s is not a valid %T", s, t)
}

func (s SummarizeRequestFormat) Ptr() *SummarizeRequestFormat {
	return &s
}

// One of `short`, `medium`, `long`, or `auto` defaults to `auto`. Indicates the approximate length of the summary. If `auto` is selected, the best option will be picked based on the input text.
type SummarizeRequestLength string

const (
	SummarizeRequestLengthShort  SummarizeRequestLength = "short"
	SummarizeRequestLengthMedium SummarizeRequestLength = "medium"
	SummarizeRequestLengthLong   SummarizeRequestLength = "long"
)

func NewSummarizeRequestLengthFromString(s string) (SummarizeRequestLength, error) {
	switch s {
	case "short":
		return SummarizeRequestLengthShort, nil
	case "medium":
		return SummarizeRequestLengthMedium, nil
	case "long":
		return SummarizeRequestLengthLong, nil
	}
	var t SummarizeRequestLength
	return "", fmt.Errorf("%s is not a valid %T", s, t)
}

func (s SummarizeRequestLength) Ptr() *SummarizeRequestLength {
	return &s
}

type SummarizeResponse struct {
	// Generated ID for the summary
	Id *string `json:"id,omitempty"`
	// Generated summary for the text
	Summary *string  `json:"summary,omitempty"`
	Meta    *ApiMeta `json:"meta,omitempty"`

	_rawJSON json.RawMessage
}

func (s *SummarizeResponse) UnmarshalJSON(data []byte) error {
	type unmarshaler SummarizeResponse
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*s = SummarizeResponse(value)
	s._rawJSON = json.RawMessage(data)
	return nil
}

func (s *SummarizeResponse) String() string {
	if len(s._rawJSON) > 0 {
		if value, err := core.StringifyJSON(s._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(s); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", s)
}

type TokenizeResponse struct {
	// An array of tokens, where each token is an integer.
	Tokens       []int    `json:"tokens,omitempty"`
	TokenStrings []string `json:"token_strings,omitempty"`
	Meta         *ApiMeta `json:"meta,omitempty"`

	_rawJSON json.RawMessage
}

func (t *TokenizeResponse) UnmarshalJSON(data []byte) error {
	type unmarshaler TokenizeResponse
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*t = TokenizeResponse(value)
	t._rawJSON = json.RawMessage(data)
	return nil
}

func (t *TokenizeResponse) String() string {
	if len(t._rawJSON) > 0 {
		if value, err := core.StringifyJSON(t._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(t); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", t)
}

type UpdateClusterJobRequestStatus string

const (
	UpdateClusterJobRequestStatusUnknown    UpdateClusterJobRequestStatus = "unknown"
	UpdateClusterJobRequestStatusProcessing UpdateClusterJobRequestStatus = "processing"
	UpdateClusterJobRequestStatusFailed     UpdateClusterJobRequestStatus = "failed"
	UpdateClusterJobRequestStatusComplete   UpdateClusterJobRequestStatus = "complete"
	UpdateClusterJobRequestStatusQueued     UpdateClusterJobRequestStatus = "queued"
)

func NewUpdateClusterJobRequestStatusFromString(s string) (UpdateClusterJobRequestStatus, error) {
	switch s {
	case "unknown":
		return UpdateClusterJobRequestStatusUnknown, nil
	case "processing":
		return UpdateClusterJobRequestStatusProcessing, nil
	case "failed":
		return UpdateClusterJobRequestStatusFailed, nil
	case "complete":
		return UpdateClusterJobRequestStatusComplete, nil
	case "queued":
		return UpdateClusterJobRequestStatusQueued, nil
	}
	var t UpdateClusterJobRequestStatus
	return "", fmt.Errorf("%s is not a valid %T", s, t)
}

func (u UpdateClusterJobRequestStatus) Ptr() *UpdateClusterJobRequestStatus {
	return &u
}

// Response for updating a cluster job.
type UpdateClusterJobResponse struct {
	JobId string `json:"job_id"`

	_rawJSON json.RawMessage
}

func (u *UpdateClusterJobResponse) UnmarshalJSON(data []byte) error {
	type unmarshaler UpdateClusterJobResponse
	var value unmarshaler
	if err := json.Unmarshal(data, &value); err != nil {
		return err
	}
	*u = UpdateClusterJobResponse(value)
	u._rawJSON = json.RawMessage(data)
	return nil
}

func (u *UpdateClusterJobResponse) String() string {
	if len(u._rawJSON) > 0 {
		if value, err := core.StringifyJSON(u._rawJSON); err == nil {
			return value
		}
	}
	if value, err := core.StringifyJSON(u); err == nil {
		return value
	}
	return fmt.Sprintf("%#v", u)
}

type UpdateClusterJobRequest struct {
	JobId                 *string                        `json:"job_id,omitempty"`
	Status                *UpdateClusterJobRequestStatus `json:"status,omitempty"`
	Clusters              []*Cluster                     `json:"clusters,omitempty"`
	OutputClustersGsPath  *string                        `json:"output_clusters_gs_path,omitempty"`
	OutputOutliersGsPath  *string                        `json:"output_outliers_gs_path,omitempty"`
	Error                 *string                        `json:"error,omitempty"`
	InputTrackingMetrics  map[string]interface{}         `json:"input_tracking_metrics,omitempty"`
	OutputTrackingMetrics map[string]interface{}         `json:"output_tracking_metrics,omitempty"`
}
